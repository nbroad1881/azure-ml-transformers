# CLM Pretraining for small language models (<2B)

This pipeline involves 3 stages:

1. Training a tokenizer
2. Tokenizing the data
3. Training the model

The notebook [run_pipeline.ipynb](run_pipeline.ipynb) walks through each step.