{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: /Users/nicholasbroad/huggingface/azure-ml-transformers/config.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "from azure.ai.ml import MLClient, command, dsl, Input, Output\n",
    "from azure.ai.ml.entities import Environment, BuildContext, AmlCompute\n",
    "from azure.identity import InteractiveBrowserCredential\n",
    "\n",
    "credential = InteractiveBrowserCredential()\n",
    "\n",
    "PATH_TO_CONFIG_FILE = \"config.json\"\n",
    "\n",
    "ml_client = MLClient.from_config(credential, path=PATH_TO_CONFIG_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPUTE_MAPPING = {\n",
    "    # Flash attention only works on GPUs that start with A (A100), L (L4, L40), or H (H100)\n",
    "    \"4xV100\": \"Standard_NC24s_v3\",\n",
    "    \"2xV100\": \"Standard_NC12s_v3\",\n",
    "    \"1xV100\": \"Standard_NC6s_v3\",\n",
    "    \"4xT4\": \"Standard_NC64as_T4_v3\",\n",
    "    \"1xT4\": \"Standard_NC4as_T4_v3\",\n",
    "    \"1xA10\": \"Standard_NV36adms_A10_v5\",\n",
    "    \"2xA10\": \"Standard_NV72ads_A10_v5\",\n",
    "    \"1xA100-80GB\": \"Standard_NC24ads_A100_v4\",\n",
    "    \"8xA100-40GB\": \"Standard_ND96asr_A100_v4\",\n",
    "    \"8xA100-80GB\": \"Standard_ND96amsr_A100_v4\",\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "timenow = datetime.utcnow().strftime(\"%Y%m%d%H%M\") # YearMonthDayHourMinute\n",
    "\n",
    "MODEL_NAME = \"sdpa_roberta_mlm__\" + timenow\n",
    "DATASTORE_NAME = \"workspaceblobstore\"\n",
    "\n",
    "\n",
    "\n",
    "TRAIN_TOKENIZER_ENV_NAME = \"train_tokenizer_env\"\n",
    "TRAIN_TOKENIZER_DIR = \"train_tokenizer\"\n",
    "TRAIN_TOKENIZER_COMPUTE_NAME = \"train-tokenization-compute\" \n",
    "TRAIN_TOKENIZER_INSTANCE_TYPE = \"Standard_F4s_v2\" \n",
    "TEXT_FILES_BLOB_PATH = \"robertamlm\"\n",
    "TEXT_FILES_PATH = f\"azureml://datastores/{DATASTORE_NAME}/paths/{TEXT_FILES_BLOB_PATH}\"\n",
    "TOKENIZER_BLOB_PATH = \"new_roberta_tokenizer\"\n",
    "TOKENIZER_OUTPUT_PATH = f\"azureml://datastores/{DATASTORE_NAME}/paths/{TOKENIZER_BLOB_PATH}\"\n",
    "TRAIN_TOKENIZER_COMMAND_NAME = \"train_tokenizer\"\n",
    "TRAIN_TOKENIZER_DISPLAY_NAME = \"Train Tokenizer\"\n",
    "\n",
    "DATA_TOKENIZATION_ENV_NAME = \"data_tokenization_env\"\n",
    "DATA_TOKENIZATION_DIR = \"data_tokenization\"\n",
    "DATA_TOKENIZATION_COMPUTE_NAME = TRAIN_TOKENIZER_COMPUTE_NAME # use same compute as training tokenizer\n",
    "DATA_TOKENIZATION_INSTANCE_TYPE = TRAIN_TOKENIZER_INSTANCE_TYPE # use same compute as training tokenizer\n",
    "TOKENIZED_DATA_BLOB_PATH = \"new_roberta_tokenized_data\"\n",
    "TOKENIZED_DATA_OUTPUT_PATH = f\"azureml://datastores/{DATASTORE_NAME}/paths/{TOKENIZED_DATA_BLOB_PATH}\"\n",
    "DATA_TOKENIZATION_COMMAND_NAME = \"data_tokenization\"\n",
    "DATA_TOKENIZATION_DISPLAY_NAME = \"Data Tokenization\"\n",
    "\n",
    "\n",
    "TRAIN_DIR = \"train_model\"\n",
    "TRAIN_ENV_NAME = \"roberta_mlm_env\"\n",
    "TRAIN_COMPUTE_NAME = \"roberta-mlm-compute-a100\"\n",
    "TRAIN_INSTANCE_TYPE = COMPUTE_MAPPING[\"1xA100-80GB\"]\n",
    "TRAIN_COMMAND_NAME = \"train\"\n",
    "TRAIN_DISPLAY_NAME = \"Train Model\"\n",
    "TRAINED_MODEL_OUTPUT_PATH = f\"azureml://datastores/{DATASTORE_NAME}/paths/{MODEL_NAME}\"\n",
    "\n",
    "PIPELINE_NAME = \"mlm_pretraining_from_scratch\"\n",
    "PIPELINE_DESCRIPTION = \"MLM pretraining pipeline from scratch\"\n",
    "EXPERIMENT_NAME = MODEL_NAME\n",
    "\n",
    "CPU_ENV_IMAGE = \"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu22.04\"\n",
    "GPU_ENV_IMAGE = \"mcr.microsoft.com/azureml/openmpi4.1.0-cuda11.8-cudnn8-ubuntu22.04\"\n",
    "\n",
    "NUM_NODES = 1\n",
    "NUM_GPUS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_compute_target(\n",
    "    ml_client,\n",
    "    compute_name,\n",
    "    instance_type=\"STANDARD_DS3_v2\",\n",
    "    min_nodes=0,\n",
    "    max_nodes=1,\n",
    "    idle_time=300,\n",
    "):\n",
    "    try:\n",
    "        cmpute = ml_client.compute.get(compute_name)\n",
    "        cmpute_name = cmpute.name\n",
    "    except Exception:\n",
    "        print(f\"Creating a new {instance_type} compute target...\")\n",
    "        compute = AmlCompute(\n",
    "            name=compute_name,\n",
    "            size=instance_type,\n",
    "            min_instances=min_nodes,\n",
    "            max_instances=max_nodes,\n",
    "            idle_time_before_scale_down=idle_time,\n",
    "        )\n",
    "        ml_client.compute.begin_create_or_update(compute)\n",
    "        cmpute_name = compute.name\n",
    "    return cmpute_name\n",
    "\n",
    "\n",
    "def get_environment(\n",
    "    environment_name,\n",
    "    dependencies_dir,\n",
    "    ml_client,\n",
    "    gpu=False,\n",
    "    dep_yaml=None,\n",
    "    dockerfile_path=None,\n",
    "):\n",
    "    try:\n",
    "        env = ml_client.environments.get(name=environment_name)\n",
    "    except Exception:\n",
    "\n",
    "        image = GPU_ENV_IMAGE if gpu else CPU_ENV_IMAGE\n",
    "\n",
    "        if dockerfile_path is not None:\n",
    "            build_context = BuildContext(\n",
    "                path=dependencies_dir, dockerfile_path=dockerfile_path\n",
    "            )\n",
    "\n",
    "            env = Environment(\n",
    "                name=environment_name,\n",
    "                description=\"Custom environment\",\n",
    "                build=build_context,\n",
    "            )\n",
    "        else:\n",
    "            env = Environment(\n",
    "                name=environment_name,\n",
    "                description=\"Custom environment\",\n",
    "                conda_file=os.path.join(dependencies_dir, dep_yaml),\n",
    "                image=image,\n",
    "            )\n",
    "\n",
    "        env = ml_client.environments.create_or_update(env)\n",
    "\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenizer_environment = get_environment(\n",
    "    environment_name=TRAIN_TOKENIZER_ENV_NAME,\n",
    "    dependencies_dir=TRAIN_TOKENIZER_DIR,\n",
    "    dep_yaml=\"env.yaml\",\n",
    "    ml_client=ml_client,\n",
    "    gpu=False,\n",
    ")\n",
    "\n",
    "train_tokenizer_compute = get_or_create_compute_target(\n",
    "    ml_client=ml_client,\n",
    "    compute_name=TRAIN_TOKENIZER_COMPUTE_NAME,\n",
    "    min_nodes=0,\n",
    "    max_nodes=1,\n",
    "    instance_type=TRAIN_TOKENIZER_INSTANCE_TYPE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenizer_command = command(\n",
    "    name=TRAIN_TOKENIZER_COMMAND_NAME,\n",
    "    display_name=TRAIN_TOKENIZER_DISPLAY_NAME,\n",
    "    inputs={\n",
    "        \"text_files_dir\": Input(\n",
    "            type=\"uri_folder\",\n",
    "            path=TEXT_FILES_PATH,\n",
    "            mode=\"mount\",\n",
    "        ),\n",
    "    },\n",
    "    outputs={\n",
    "        \"output_dir\": Output(\n",
    "            type=\"uri_folder\",\n",
    "            path=TOKENIZER_OUTPUT_PATH,\n",
    "            mode=\"rw_mount\",\n",
    "        ),\n",
    "    },\n",
    "    # The source folder of the component\n",
    "    code=\"./train_tokenizer\",\n",
    "    command=\"\"\"\n",
    "python run.py \\\n",
    "    --tokenizer_name \"FacebookAI/roberta-base\" \\\n",
    "    --vocab_size 100000 \\\n",
    "    --text_files_dir ${{inputs.text_files_dir}} \\\n",
    "    --glob_pattern \"*.parquet\" \\\n",
    "    --num_samples 10000 \\\n",
    "    --output_dir ${{outputs.output_dir}}\n",
    "            \"\"\",\n",
    "    environment=f\"{train_tokenizer_environment.name}:{train_tokenizer_environment.version}\",\n",
    "    compute=train_tokenizer_compute,\n",
    "    instance_count=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tokenization_environment = get_environment(\n",
    "    environment_name=DATA_TOKENIZATION_ENV_NAME,\n",
    "    dependencies_dir=DATA_TOKENIZATION_DIR,\n",
    "    ml_client=ml_client,\n",
    "    dep_yaml=\"env.yaml\",\n",
    "    gpu=False,\n",
    ")\n",
    "\n",
    "data_tokenization_compute = get_or_create_compute_target(\n",
    "    ml_client=ml_client,\n",
    "    compute_name=DATA_TOKENIZATION_COMPUTE_NAME,\n",
    "    min_nodes=0,\n",
    "    max_nodes=1,\n",
    "    instance_type=DATA_TOKENIZATION_INSTANCE_TYPE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tokenization_command = command(\n",
    "    name=\"data_tokenization\",\n",
    "    display_name=\"Data Tokenization\",\n",
    "    inputs={\n",
    "        \"text_files_dir\": Input(\n",
    "            type=\"uri_folder\",\n",
    "            path=TEXT_FILES_PATH,\n",
    "            mode=\"mount\",\n",
    "        ),\n",
    "        \"tokenizer_name_or_path\": Input(\n",
    "            type=\"uri_folder\",\n",
    "            path=TOKENIZER_OUTPUT_PATH,\n",
    "            mode=\"mount\",\n",
    "        ),\n",
    "    },\n",
    "    outputs={\n",
    "        \"output_dir\": Output(\n",
    "            type=\"uri_folder\",\n",
    "            path=TOKENIZED_DATA_OUTPUT_PATH,\n",
    "            mode=\"rw_mount\",\n",
    "        ),\n",
    "    },\n",
    "    # The source folder of the component\n",
    "    code=\"./data_tokenization\",\n",
    "    command=\"\"\"\n",
    "python run.py \\\n",
    "    --tokenizer_name_or_path ${{inputs.tokenizer_name_or_path}} \\\n",
    "    --file_type \"parquet\" \\\n",
    "    --text_files_dir ${{inputs.text_files_dir}} \\\n",
    "    --glob_pattern \"*.parquet\" \\\n",
    "    --output_dir ${{outputs.output_dir}} \\\n",
    "    --num_proc 4\n",
    "            \"\"\",\n",
    "    environment=f\"{data_tokenization_environment.name}:{data_tokenization_environment.version}\",\n",
    "    compute=data_tokenization_compute,\n",
    "    instance_count=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_environment = get_environment(\n",
    "    environment_name=TRAIN_ENV_NAME,\n",
    "    dependencies_dir=TRAIN_DIR,\n",
    "    dep_yaml=\"env.yaml\",\n",
    "    ml_client=ml_client,\n",
    "    gpu=True,\n",
    ")\n",
    "\n",
    "train_compute = get_or_create_compute_target(\n",
    "    ml_client=ml_client,\n",
    "    compute_name=TRAIN_COMPUTE_NAME,\n",
    "    min_nodes=0,\n",
    "    max_nodes=1,\n",
    "    instance_type=TRAIN_INSTANCE_TYPE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Since I use a custom mlflow callback, I disable `report_to` in the training command.\n",
    "\"\"\"\n",
    "\n",
    "train_command = command(\n",
    "    name=TRAIN_COMMAND_NAME,\n",
    "    display_name=TRAIN_DISPLAY_NAME,\n",
    "    inputs={\n",
    "        \"num_processes\": NUM_GPUS,\n",
    "        \"tokenizer_name_or_path\" : Input(\n",
    "            type=\"uri_folder\",\n",
    "            path=TOKENIZER_OUTPUT_PATH,\n",
    "            mode=\"mount\",\n",
    "        ),\n",
    "        \"tokenized_files_dir\": Input(\n",
    "            type=\"uri_folder\",\n",
    "            path=TOKENIZED_DATA_OUTPUT_PATH,\n",
    "            mode=\"mount\",\n",
    "        ),\n",
    "    },\n",
    "    outputs={\n",
    "        \"output_dir\": Output(\n",
    "            type=\"uri_folder\",\n",
    "            path=TRAINED_MODEL_OUTPUT_PATH,\n",
    "            mode=\"rw_mount\",\n",
    "        ),\n",
    "    },\n",
    "    \n",
    "    code=\"./train_model\",\n",
    "    command=\"\"\"\n",
    "accelerate launch --num_processes ${{inputs.num_processes}} --num_machines 1 \\\n",
    "    run.py \\\n",
    "--tokenizer_name_or_path ${{inputs.tokenizer_name_or_path}} \\\n",
    "--config_name_or_path roberta-base \\\n",
    "--tokenized_files_dir ${{inputs.tokenized_files_dir}} \\\n",
    "--glob_pattern \"*.parquet\" \\\n",
    "--output_dir ${{outputs.output_dir}} \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--eval_strategy epoch \\\n",
    "--validation_split_num_samples_or_percentage 1000 \\\n",
    "--warmup_steps 100 \\\n",
    "--fp16 \\\n",
    "--masking_probability 0.15 \\\n",
    "--attn_implementation sdpa \\\n",
    "--per_device_train_batch_size 32 \\\n",
    "--per_device_eval_batch_size 32 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--gradient_checkpointing False \\\n",
    "--num_train_epochs 3 \\\n",
    "--learning_rate 5e-5 \\\n",
    "--weight_decay 0.01 \\\n",
    "--optim adamw_torch \\\n",
    "--logging_steps 10 \\\n",
    "--save_strategy epoch \\\n",
    "--save_total_limit 3 \\\n",
    "--report_to none \\\n",
    "--torch_compile False \\\n",
    "--dataloader_num_workers 2 \\\n",
    "--ddp_find_unused_parameters False \\\n",
    "--max_steps 125\n",
    "\"\"\",\n",
    "    environment=f\"{train_environment.name}:{train_environment.version}\",\n",
    "    compute=train_compute,\n",
    "    shm_size=\"16g\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mUploading train_model (0.1 MBs): 100%|██████████| 97509/97509 [00:01<00:00, 55632.23it/s]\n",
      "\u001b[39m\n",
      "\n",
      "pathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFolderJobOutput'> and will be ignored\n",
      "pathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFolderJobOutput'> and will be ignored\n",
      "pathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFolderJobOutput'> and will be ignored\n"
     ]
    }
   ],
   "source": [
    "@dsl.pipeline(\n",
    "    description=PIPELINE_DESCRIPTION,\n",
    "    display_name=PIPELINE_NAME,\n",
    ")\n",
    "def pipeline_func():\n",
    "\n",
    "    train_tokenizer_job = train_tokenizer_command()\n",
    "\n",
    "    data_tokenization_job = data_tokenization_command(tokenizer_name_or_path=train_tokenizer_job.outputs.output_dir)\n",
    "\n",
    "    train_job = train_command(\n",
    "        tokenizer_name_or_path=train_tokenizer_job.outputs.output_dir,\n",
    "        tokenized_files_dir=data_tokenization_job.outputs.output_dir,\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        \"pipeline_job_train_data\": train_job.outputs.output_dir,\n",
    "    }\n",
    "\n",
    "\n",
    "pipeline = pipeline_func()\n",
    "\n",
    "\n",
    "pipeline_job = ml_client.jobs.create_or_update(\n",
    "    pipeline,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
